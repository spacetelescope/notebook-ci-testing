name: Domain Accessibility Crawler

on:
  workflow_dispatch:
    inputs:
      domain:
        description: 'Domain to scan (e.g., https://spacetelescope.github.io)'
        required: true
        default: 'https://spacetelescope.github.io'
      path_pattern:
        description: 'URL path pattern to scan (e.g., /hst_notebooks/notebooks)'
        required: false
        default: '/hst_notebooks'
      output_report:
        description: 'Path to save combined report'
        required: false
        default: 'accessibility-report.json'

jobs:
  discover-pages:
    name: Discover Webpages
    runs-on: ubuntu-latest
    outputs:
      pages: ${{ steps.discover.outputs.pages }}
      page-count: ${{ steps.discover.outputs.page-count }}
    steps:
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4

      - name: Create page discovery script
        run: |
          cat > crawl.py << 'PYSCRIPT'
          import sys
          import requests
          from bs4 import BeautifulSoup
          from urllib.parse import urljoin, urlparse
          import json
          
          domain = sys.argv[1]
          path_pattern = sys.argv[2] if len(sys.argv) > 2 else "/"
          
          visited = set()
          pages = []
          to_visit = [domain.rstrip('/') + path_pattern.rstrip('/') + '/']
          
          def is_valid_url(url, base_domain):
              parsed = urlparse(url)
              base_parsed = urlparse(base_domain)
              return parsed.netloc == base_parsed.netloc
          
          max_pages = 100
          
          while to_visit and len(pages) < max_pages:
              try:
                  url = to_visit.pop(0)
                  if url in visited:
                      continue
                  
                  visited.add(url)
                  
                  headers = {
                      'User-Agent': 'Mozilla/5.0 (compatible; AccessibilityBot/1.0)'
                  }
                  response = requests.get(url, timeout=10, headers=headers)
                  response.raise_for_status()
                  
                  if 'text/html' in response.headers.get('Content-Type', ''):
                      pages.append(url)
                      soup = BeautifulSoup(response.content, 'html.parser')
                      
                      for link in soup.find_all('a', href=True):
                          href = urljoin(url, link['href'])
                          href = href.split('#')[0]
                          
                          if is_valid_url(href, domain) and href not in visited:
                              to_visit.append(href)
              
              except Exception as e:
                  print(f"Error processing {url}: {e}", file=sys.stderr)
          
          print(json.dumps(pages))
          PYSCRIPT

      - name: Discover all HTML pages
        id: discover
        run: |
          DOMAIN="${{ github.event.inputs.domain }}"
          PATH_PATTERN="${{ github.event.inputs.path_pattern }}"
          
          python3 crawl.py "$DOMAIN" "$PATH_PATTERN" > pages.json
          PAGES=$(cat pages.json)
          PAGE_COUNT=$(echo "$PAGES" | python3 -c "import sys, json; print(len(json.load(sys.stdin)))")
          
          echo "pages=$PAGES" >> $GITHUB_OUTPUT
          echo "page-count=$PAGE_COUNT" >> $GITHUB_OUTPUT
          
          echo "Discovered $PAGE_COUNT pages"
          echo "$PAGES" | python3 -m json.tool

  accessibility-check:
    name: Check Accessibility
    needs: discover-pages
    runs-on: ubuntu-latest
    strategy:
      matrix:
        page: ${{ fromJson(needs.discover-pages.outputs.pages) }}
      max-parallel: 5
    steps:
      - uses: actions/checkout@v4

      - name: Run be-a11y Accessibility Checker
        id: a11ychecker
        uses: be-lenka/be-a11y@v2.2.8
        continue-on-error: true
        with:
          url: ${{ matrix.page }}
          report: accessibility-report-${{ strategy.job-index }}.json

      - name: Upload individual report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: accessibility-reports-${{ github.run_id }}
          path: accessibility-report-${{ strategy.job-index }}.json


      - name: Comment on issues
        if: steps.a11ychecker.outcome != 'success'
        run: |
          echo "Accessibility issues found on: ${{ matrix.page }}"

  consolidate-reports:
    name: Consolidate Reports
    needs: accessibility-check
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Download all reports
        uses: actions/download-artifact@v4
        with:
          name: accessibility-reports-${{ github.run_id }}
          path: reports

      - name: Create consolidation script
        run: |
          cat > consolidate.py << 'PYSCRIPT'
          import json
          import os
          import sys
          from pathlib import Path
          
          reports_dir = 'reports'
          output_file = '${{ github.event.inputs.output_report }}'
          
          consolidated = {
              "summary": {
                  "total_pages": 0,
                  "pages_with_issues": 0,
                  "total_issues": 0
              },
              "pages": []
          }
          
          for report_file in sorted(Path(reports_dir).glob('*.json')):
              try:
                  with open(report_file, 'r') as f:
                      report = json.load(f)
                      consolidated["pages"].append(report)
              except Exception as e:
                  print(f"Error processing {report_file}: {e}", file=sys.stderr)
          
          consolidated["summary"]["total_pages"] = len(consolidated["pages"])
          consolidated["summary"]["pages_with_issues"] = sum(
              1 for p in consolidated["pages"] 
              if p.get("summary", {}).get("issueCount", 0) > 0
          )
          consolidated["summary"]["total_issues"] = sum(
              p.get("summary", {}).get("issueCount", 0) 
              for p in consolidated["pages"]
          )
          
          with open(output_file, 'w') as f:
              json.dump(consolidated, f, indent=2)
          
          print(f"Consolidated {len(consolidated['pages'])} reports")
          print(f"Total issues: {consolidated['summary']['total_issues']}")
          PYSCRIPT

      - name: Consolidate reports
        run: python3 consolidate.py

      - name: Upload consolidated report
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-accessibility-report-${{ github.run_id }}
          path: ${{ github.event.inputs.output_report }}


      - name: Summary
        if: always()
        run: |
          echo "Accessibility Check Summary"
          echo "==========================="
          echo "Report: ${{ github.event.inputs.output_report }}"
          echo "Total pages scanned: ${{ needs.discover-pages.outputs.page-count }}"
          echo ""
          echo "Download the consolidated report from artifacts."

