{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc51250",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Classifying Pan-STARRS sources with unsupervised and supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f9875",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3669a91",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "\n",
    "**In this tutorial, you will see an example of using models to classify sources extracted from astronomical data.**  By the end of this tutorial you will have working examples of how to apply principal component analysis (PCA), t-distributed stochastic neighbor embedding (tSNE), k-means clustering (unsupervised learning) and stochastic gradient descent (SGD) classification (supervised learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a41d2af",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Machine learning which can be applied in both supervised and unsupervised contexts for classification. In this notebook, we will walk through the basic steps of reducing the dimensionality of astronomical source catalogs with PCA and tSNE and classifying the sources in unsupervised and supervised formats.\n",
    "\n",
    "1. Download and import the data\n",
    "2. Perform data cleaning steps\n",
    "3. Reduce dimensionality with PCA and tSNE\n",
    "4. Perform unsupervised clustering with k-means\n",
    "5. Perform supervised classification with SGD\n",
    "6. Assess the success of the model with a confusion matrix\n",
    "\n",
    "Here we will classify sources extracted from multi-wavelength photometry from the [Pan-STARRS](https://outerspace.stsci.edu/display/PANSTARRS/) survey, and compare with previous classifications from a convolutional neural network analysis by the [PS1-STRM](https://archive.stsci.edu/hlsp/ps1-strm) team.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9813d",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This notebook uses the following:\n",
    "- `numpy` to handle array functions\n",
    "- `astropy` for accessing FITS files\n",
    "- `matplotlib.pyplot` for plotting data\n",
    "- `sklearn` for models and performance metrics\n",
    "- `itertools` for generating combinations\n",
    "- `scipy` for statistics\n",
    "\n",
    "If you do not have these packages installed, you can install them using [`pip`](https://pip.pypa.io/en/stable/) or [`conda`](https://docs.conda.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f824294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrays\n",
    "import numpy as np\n",
    "\n",
    "# fits\n",
    "from astropy.io import fits\n",
    "from astropy.utils.data import download_file\n",
    "from astropy.table import Table\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn for performance metrics\n",
    "\n",
    "# sklearn \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "# iterative combinations\n",
    "from itertools import combinations\n",
    "\n",
    "# statistics \n",
    "import scipy.stats as st\n",
    "\n",
    "# from IPython import get_ipython\n",
    "# get_ipython().run_line_magic('matplotlib', 'notebook')\n",
    "\n",
    "# set random seed for reproducibility \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4ccdc",
   "metadata": {},
   "source": [
    "### 1. Download the data\n",
    "\n",
    "For the following exercises, we will use data from the Pan-STARRS1 (PS1) catalog, which includes wide-area imaging in five wavelength bands, g, r, i, z  and y.\n",
    "\n",
    "Using convolutional neural networks (CNNs) trained on spectroscopic surveys, the Pan-STARRS1 Source Types and Redshifts with Machine Learning (PS1-STRM) ([Beck et al. 2020](https://dx.doi.org/10.1093/mnras/staa2587)) team classified (\"star\", \"galaxy\", \"QSO\" and \"unsure\") and regressed redshift information for the 2,902,054,648 discree sources in the [PS1 3$\\pi$ DR1 catalog](https://outerspace.stsci.edu/display/PANSTARRS/PS1+Source+extraction+and+catalogs). This volume of data is far too large to analyze in a short notebook example. Therefore, we extracted a subset of PS1-STRM and cross-matched it with the PS1 catalog (see Appendix for details on how to obtain this subset from MAST). \n",
    "\n",
    "This subset of PS1 will be used to test whether simple clustering algorithms can reproduce the classification results of PS1-STRM.  The dataset consists of 100,000 sources, randomly selected to have roughly equal representation in the four PS1-STRM classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a4298",
   "metadata": {},
   "source": [
    "First, load the data into an `astropy` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78255c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file_url = 'https://archive.stsci.edu/hlsps/hellouniverse/hellouniverse_ps1strm_subset.fits'\n",
    "tab = Table.read(download_file(file_url, cache=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d4fb0",
   "metadata": {},
   "source": [
    "Inspect the contents of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55110fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea67722",
   "metadata": {},
   "source": [
    "### 2. Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172fe5a4",
   "metadata": {},
   "source": [
    "As our interest is to classify the sources, we first \"encode\" the PS1-STRM classifications from string values to integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd674b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "tab['class_encode'] = label_encoder.fit_transform(tab['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed02d0",
   "metadata": {},
   "source": [
    "Next, we will select a subset of the table, specifically the \"FPSFMag\" magnitude values for the five PS1 bands (g, r, i z, and y). Following the strategy of [Beck et al. 2020](https://dx.doi.org/10.1093/mnras/staa2587), we will normalize these magnitudes by subtracting the median. Then, we will impute missing values by setting them equal to the median. **NOTE** this may be a risky move! An alternative strategy is to simply remove the missing values from the training set, or to impose an *N*-$\\sigma$ limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17710e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = ['g', 'r', 'i', 'z', 'y']\n",
    "tt = 'FPSFMag'\n",
    "\n",
    "tags_f = np.ravel([[b+tt] for b in bands])\n",
    "fluxes = [tab[tag] for tag in tags_f]\n",
    "for k in range(len(tags_f)):\n",
    "    fluxes[k] = fluxes[k] - np.median(fluxes[k]) \n",
    "    \n",
    "    fluxes[k] = np.where(fluxes[k] < -100, np.median(fluxes[k]), fluxes[k])\n",
    "    fluxes[k] = np.where(fluxes[k] > 100, np.median(fluxes[k]), fluxes[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38703835",
   "metadata": {},
   "source": [
    "In addition to measured fluxes, we will add galaxy colors to the sample. For the unique combinations of filters we will compute colors. As for the fluxes, we normalize by subtracting the medians, and then all missing values are set to the medians as above (but see note!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca87b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = list(combinations(bands, 2))\n",
    "colors = []\n",
    "tags_c = []\n",
    "for c in list(combos): \n",
    "    color = np.array(tab[c[0]+tt]-tab[c[1]+tt])\n",
    "    \n",
    "    color = color - np.median(color)\n",
    "    color[color < -100] = np.median(color)\n",
    "    color[color > 100] = np.median(color)\n",
    "\n",
    "    colors.append(color)\n",
    "    tags_c.append(c[0]+','+c[1]+','+tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c12b98",
   "metadata": {},
   "source": [
    "We then combine the flux sample with the color sample to produce a 100000 x 15 catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e29a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array(np.vstack([fluxes, colors]))\n",
    "sample = np.array(sample).T\n",
    "\n",
    "np.shape(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75fd3b4",
   "metadata": {},
   "source": [
    "### 3. Reduce dimensionality with PCA and tSNE\n",
    "\n",
    "To explore how our selected features recover the classifications from PS1-STRM, we will apply PCA to the full sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa77fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sample = PCA(n_components=2).fit_transform(sample)\n",
    "print(pca_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e5083f",
   "metadata": {},
   "source": [
    "The shape of the reduced PCA sample is now 100000 x 2. To visualize the results, we plot the reduced sample and over-plot the populations for each of the four PS1 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['GALAXY', 'QSO', 'STAR', 'UNSURE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = -6, 16\n",
    "ymin, ymax = -6, 4\n",
    "\n",
    "# Peform the kernel density estimate\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "colors = ['dodgerblue', 'orange', 'limegreen', 'magenta']\n",
    "for k, c in enumerate(classes):\n",
    "    spots = tab['class'] == c\n",
    "    \n",
    "    plt.subplot(2, 2, k+1)\n",
    "    plt.scatter(pca_sample[:, 0], pca_sample[:, 1], s=2, marker='.', color='grey', alpha=0.01)# bins='log', cmap='Greys', alpha=0.5) \n",
    "    \n",
    "    values = np.vstack([pca_sample[:, 0][spots], pca_sample[:, 1][spots]])\n",
    "    kernel = st.gaussian_kde(values)\n",
    "    f = np.reshape(kernel(positions).T, xx.shape)\n",
    "    cntr = plt.contour(xx, yy, f, colors=colors[k])\n",
    "    h1, _ = cntr.legend_elements()\n",
    "    \n",
    "    plt.legend([h1[0]], [c])\n",
    "    plt.xlim(-7, 16)\n",
    "    plt.ylim(-6, 6)\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09721eb",
   "metadata": {},
   "source": [
    "By inspection of the above panels, it looks like the different classes occupy unique regions of the simple PCA parameter space, with considerable overlap. As a next test, we will use tSNE to reduce dimensionality of the original dataset. In this example we will randomly subset the sample by 10% so that the tSNE algorithm runs quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "idx = np.random.choice(sample.shape[0], 10000)\n",
    "tsne_sample = TSNE(n_components=2, init='pca', perplexity=10, learning_rate = 10).fit_transform(sample[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0255e71",
   "metadata": {},
   "source": [
    "And similar to the PCA analysis, we will visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf47797",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = -45, 45\n",
    "ymin, ymax = -45, 45\n",
    "\n",
    "# Peform the kernel density estimate\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for k, c in enumerate(classes):\n",
    "    spots = tab['class'][idx] == c\n",
    "    plt.subplot(2, 2, k+1)\n",
    "    plt.scatter(tsne_sample[:, 0], tsne_sample[:, 1], c='black', s=2, alpha=0.01)\n",
    "    \n",
    "    values = np.vstack([tsne_sample[:, 0][spots], tsne_sample[:, 1][spots]])\n",
    "    kernel = st.gaussian_kde(values)\n",
    "    f = np.reshape(kernel(positions).T, xx.shape)\n",
    "    cntr = plt.contour(xx, yy, f, colors=colors[k]) \n",
    "    h1, _ = cntr.legend_elements()\n",
    "    \n",
    "    plt.legend([h1[0]], [c])\n",
    "    plt.title(c)\n",
    "    plt.xlabel('tSNE Dimension 1')\n",
    "    plt.ylabel('tSNE Dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ac074",
   "metadata": {},
   "source": [
    "Again, it looks like each class occupies a distinct region of tSNE parameter space, although it is a bit noisy with lots of overlap, especially between the \"galaxy\" and \"unsure\" classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed178e8d",
   "metadata": {},
   "source": [
    "### 4. Perform unsupervised classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43296f6a",
   "metadata": {},
   "source": [
    "Next we will perform unsupervised classification with clustering. There are many choices of algorithms (see the [sklearn clustering documentation](https://scikit-learn.org/stable/modules/clustering.html)), and here we will apply the [k-means](https://scikit-learn.org/stable/modules/clustering.html#k-means) algorithm, which requires that we specify the number of clusters a priori (4 in this case):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156f862",
   "metadata": {},
   "source": [
    "First, we will split the sample into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82996e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tab['class_encode']\n",
    "\n",
    "# Split off 30% of the data for testing\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(sample, labels, np.arange(len(labels)), test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb26c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(init=\"k-means++\", n_clusters=4)\n",
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585dde17",
   "metadata": {},
   "source": [
    "To visualize the results of the unsupervised clustering model, we will use the PCA representation of the **test** dataset and color the points by their labels assigned by k-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "xmin, xmax = -10, 20\n",
    "ymin, ymax = -10, 20\n",
    "\n",
    "# Set up a grid for the kernel density estimate\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "pca_sample_test = pca_sample[idx_test]\n",
    "kmeans_labels = kmeans.predict(X_test)\n",
    "\n",
    "for k in range(4):\n",
    "    pca_cluster = pca_sample_test[kmeans_labels == k]\n",
    "    \n",
    "    values = np.vstack([pca_cluster[:, 0], pca_cluster[:, 1]])\n",
    "    kernel = st.gaussian_kde(values)\n",
    "    f = np.reshape(kernel(positions).T, xx.shape)\n",
    "    cntr = plt.contour(xx, yy, f, colors=colors[k], levels = np.linspace(0.01,0.12,6))\n",
    "\n",
    "plt.title(\"Unsupervised Clustering\\nk-means\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.xlim(-7, 16)\n",
    "plt.ylim(-6, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def675d",
   "metadata": {},
   "source": [
    "When given the number of clusters, the k-means algorithm does a reasonable job of identifying distinct clusters. However, comparing these results with the true locations of the \"galaxy\", \"qso\", \"star\" and \"unsure\" classes in the PCA figure in Section 3, we can see that there is not a perfect correspondence between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717817b0",
   "metadata": {},
   "source": [
    "### 5. Perform supervised clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e3794",
   "metadata": {},
   "source": [
    "Given the class labels from the CNN analysis by [Beck et al. 2020](https://dx.doi.org/10.1093/mnras/staa2587), we will also apply supervised classification (i.e., training the algorithm using a sample of known answers). Here we'll use [stochastic gradient descent (SGD)](https://scikit-learn.org/stable/modules/sgd.html#sgd). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59833c5b",
   "metadata": {},
   "source": [
    "Initialize the SGD classifier via sklearn. Here we will use the default parameters, but please see [the SGDClassifier() documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) for more information on the available options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ba618",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55986ee0",
   "metadata": {},
   "source": [
    "To visualize the results of the supervised classification model, we will use the PCA representation of the **test** dataset and color the points by their labels assigned by SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19957f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "xmin, xmax = -10, 20\n",
    "ymin, ymax = -10, 20\n",
    "\n",
    "# Peform the kernel density estimate\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "pca_sample_test = pca_sample[idx_test]\n",
    "sgd_labels = sgd.predict(X_test)\n",
    "\n",
    "for k in range(4):\n",
    "    pca_cluster = pca_sample_test[sgd_labels == k]\n",
    "    \n",
    "    values = np.vstack([pca_cluster[:, 0], pca_cluster[:, 1]])\n",
    "    kernel = st.gaussian_kde(values) \n",
    "    f = np.reshape(kernel(positions).T, xx.shape)\n",
    "    cntr = plt.contour(xx, yy, f, colors=colors[k], levels = np.linspace(0.01,0.12,6))\n",
    "\n",
    "\n",
    "plt.title(\"Supervised Classification\\nSGD\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.xlim(-7, 16)\n",
    "plt.ylim(-6, 6)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81014da7",
   "metadata": {},
   "source": [
    "Unlike the unsupervised k-means classifier, the supervised SGD classifier appears to do a better job of identifying the true classifications (see Section 3), especially in the case of overlapping clusters in the PCA representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab6aff",
   "metadata": {},
   "source": [
    "### 6. Assess the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdffac45",
   "metadata": {},
   "source": [
    "To visulize the performance of the supervised and unsupervised model results, we will use a [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638731d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(predictions, input_data, input_labels):\n",
    "    \n",
    "    # Compute the confusion matrix by comparing the test labels (ds.test_labels) with the test predictions\n",
    "    cm = metrics.confusion_matrix(input_labels, predictions, labels=[0, 1, 2, 3])\n",
    "    cm = cm.astype('float')\n",
    "\n",
    "    # Normalize the confusion matrix results. \n",
    "    cm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plotting\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.matshow(cm_norm)\n",
    "\n",
    "    plt.title('Confusion matrix', y=1.08)\n",
    "    \n",
    "    ax.set_xticks([0, 1, 2, 3])\n",
    "    ax.set_xticklabels(['Galaxy', 'QSO', 'Star', 'Unsure'])\n",
    "    \n",
    "    ax.set_yticks([0, 1, 2, 3])\n",
    "    ax.set_yticklabels(['Galaxy', 'QSO', 'Star', 'Unsure'])\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i in range(cm_norm.shape[0]):\n",
    "        for j in range(cm_norm.shape[1]):\n",
    "            ax.text(j, i, format(cm_norm[i, j], fmt), \n",
    "                    ha=\"center\", va=\"center\", \n",
    "                    color=\"white\" if cm_norm[i, j] < thresh else \"black\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a66dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(kmeans_labels, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941e9084",
   "metadata": {},
   "source": [
    "From the above plot, it is clear that the unsupervised clustering algorithm does an extremely poor job of identifying the correct classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c24fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(sgd_labels, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668812cf",
   "metadata": {},
   "source": [
    "Based on the above confusion matrix, the SGD classifier performs very well at identifying stars (~94% true positive) whereas it does not perform as well at distinguishing between the galaxy, QSO and unsure classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0591d8de",
   "metadata": {},
   "source": [
    "## FAQs\n",
    "\n",
    "- **How do I interpret these results?** The analysis above indicates that meaningful information about unresolved astronomical sources can be extracted from their simple measured fluxes and colors. In particular, the SGD classifier is able to identify stars well, even with the out-of-the-box parameters from sklearn. However, unsupervised k-means clustering is not able to successfully classify sources.\n",
    "\n",
    "\n",
    "- **Can I improve the model by changing it?** Yes! In the k-means clustering and SGD classifier cases, there are many model parameters which can be tuned to improve the performance of the model. For example, in SGD, you can select the loss function (here we use the default, \"hinge\" which gives a linear [support vector machine](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html).\n",
    "\n",
    "\n",
    "- **Can I try a different model?  I think the results could be improved.** Yes! Check out the documentation at sklearn about classification. In particular, the [sklearn flowchart](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) is a useful tool for figuring out which (of the many!) options might be useful. \n",
    "\n",
    "\n",
    "- **I want to test my model on my training data!** No. You will convince yourself that your results are much better than they actually are.  Always keep your training, validation, and testing sets completely separate!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b82b5e",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "\n",
    "**How to extract the subset of data analyzed here from the PS1 and PS1-STRM catalogs**\n",
    "\n",
    "Instead of working with the full PS1-STRM catalog, we extracted a subset of 100,000 sources.\n",
    "\n",
    "First, we downloaded a single [PS1-STRM](https://archive.stsci.edu/hlsp/ps1-strm) file (all sources with Declinations between 77 and 90) from MAST. We then randomly selected objects so that each of the four classes (\"galaxy\", \"qso\", \"star\", \"unsure\") had equal representation in a sample of 100,000 objects. We then saved the object IDs and unique internal object identifiers to a csv catalog file. \n",
    "\n",
    "Then, we uploaded this file to [MAST CasJobs](http://mastweb.stsci.edu/mcasjobs/) as \"MyTable\". We first cross-matched the object IDs we selected with the PS1-STRM catalog via the following query:\n",
    "\n",
    "\n",
    "\n",
    "*select t.objid, s.class, s.prob_Galaxy, s.prob_Star, s.prob_QSO, s.z_phot, s.z_photErr* \\\n",
    "*into MyDB.ps1strm_fmo1* \\\n",
    "*from MyDB.MyTable t* \\\n",
    "*join catalogRecordRowStore s on s.objid=t.objid and s.uniquePspsOBid=t.uniquePspsOBid*\n",
    "\n",
    "\n",
    "We then cross-matched this catalog with the full PS1 ForcedMeanObject table with the following query:\n",
    "\n",
    "\n",
    "\n",
    "*select s.class, s.prob_Galaxy, s.prob_Star, s.prob_QSO, s.z_phot, s.z_photErr, fmo.* \\\n",
    "*into MyDB.ps1strm_fmo* \\\n",
    "*from MyDB.ps1strm_fmo1 s* \\\n",
    "*join ForcedMeanObject fmo on fmo.objid=s.objid*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c2acc",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "\n",
    "**Author:**  \n",
    "Claire Murray, Assistant Astronomer, cmurray1@stsci.edu  \n",
    "\n",
    "\n",
    "**Info:**  \n",
    "This notebook is based on the PS1-STRM catalog ([Beck et al. 2020](https://dx.doi.org/10.1093/mnras/staa2587)).\n",
    "\n",
    "**Updated On:** 2022-5-25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60053a",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "If you use this data set, `astropy`, or `sklearn` for published research, please cite the\n",
    "authors. Follow these links for more information:\n",
    "\n",
    "* [Citing the data set](https://ui.adsabs.harvard.edu/abs/2021MNRAS.500.1633B/abstract)\n",
    "* [Citing `astropy`](https://www.astropy.org/acknowledging.html)\n",
    "* [Citing `sklearn`](http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10703b3",
   "metadata": {},
   "source": [
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
